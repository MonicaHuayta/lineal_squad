---
title: "Examen Final"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(MASS)
library(car)
library("PerformanceAnalytics")
library(lmtest)
library(glmnet)

```

## **Pregunta 3** 
### *Regresion lineal SWISS*
#### *Descripcion de datos*

* $\textbf{Fertility }$ Ig. medida común de fertilidad estandarizada

* $\textbf{Agriculture }$ % de hombres involucrados en la agricultura como ocupacion

* $\textbf{Examination }$ % de reclutas que reciben la calificacion mas alta en el examen del ejercito

* $\textbf{Education }$ % educacion mas allá de la escuela primaria para reclutas.

* $\textbf{Catholic }$ % 'catolico' (a diferencia de 'protestante').

* $\textbf{Infant.Mortality }$ nacidos vivos que viven menos de 1 anhio.

```{r code1, exercise=TRUE} 
data(swiss)
colnames(swiss)
str(swiss)
summary(swiss)
```

```{r code2, exercise=TRUE}
hist(swiss$Agriculture , col="#01FF70", main="", xlab="Agriculture", ylab="Frecuencia")
hist(swiss$Examination , col="#01FF70", main="", xlab="Examination", ylab="Frecuencia")
hist(swiss$Education , col="#01FF70", main="", xlab="Education", ylab="Frecuencia")
hist(swiss$Catholic , col="#01FF70", main="", xlab="Catholic", ylab="Frecuencia")
hist(swiss$Infant.Mortality, col="#01FF70", main="", xlab="Mortality", ylab="Frecuencia")
hist(swiss$Fertility , col="#01FF70", main="", xlab="Fertilidad", ylab="Frecuencia")
```

#### *Regresion lineal*

```{r code3, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
summary(lm1)

```

Lo primera observación es que tenemos un buen modelo y hay varias variables (la mayoría) con una buena
significancia. Un punto que no debería suceder pero siempre ocurre es que el intercepto no es 0, esto podría
causar que los valores estimados de Fertilidad salgan menores a 0, lo cual en la realidad sería imposible pues no
se puede recaudar negativo.

```{r code4, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
lm1$coefficients
```
Lo que significan estos coeficientes, por ejemplo el último de la lista Infant.Mortality nos dice que si todas las otros
otros regresores se mantuvieran constantes y solamente ese cambiara en una unidad, el Fertility se vería
afectado en 1.07705. 


#### *Analizado los graficos de la regresion, Valores reales vs valores estimados*

```{r code5, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
plot(lm1$fitted.values, swiss$Fertility)
abline(0, 1, col = 2)
```

#### Supuestos

**Grafico de los residuos - homocedasticidad**

La figura nos permite evaluar la hipotesis de homocedasticidad de los residuos,
sera aceptable cuando la anchura horizontal del grafico se mantenga razonablemente
constante. 

Si el p-value del test de Breusch-Pagan es mayor que 0.05 entonces aceptamos la hipotesis
nula (la varianza de todos los grupos son iguales) y decimos que cumple el
supuesto de homocedasticidad, caso contrario no se cumple el supuesto de homocedasticidad
de varianzas. Podemos ver que se acepta la homocedasticidad.


```{r}
lm1 <- lm(formula = Fertility ~ ., data = swiss)

ncvTest(lm1)

plot(lm1, which=c(3))
```


**Efecto de las variables independientes en la variable dependiente es lineal**

```{r}
par(mfrow=c(1,1))
plot(lm1, which=c(1))

```

**Normalidad de los errores**

Los errores residuales son normales y normalmente distribuidos con media cero.
Aplicando el test de Shapiro-Wilk se probara si se cumple o no el supuesto de normalidad de los
residuos. Con un valor de 0.93 probamos la normalidad de los datos.


```{r}

shapiro.test(lm1$residuals)

par(mfrow=c(1,1))
plot(lm1, which=c(2))

```
**Los errores no estan correlacionados**

Autocorrelación significa la correlación de los valores de una misma variable ordenados en el tiempo (con datos de
series temporales) o en el espacio (con datos espaciales). 

```{r}

dwtest(lm1)

par(mfrow=c(1,1))
plot(lm1, which=c(3))

```
**Multicolinealidad de los regresores es minima**

Buscaremos variables que estén bastante correlacionadas con las demás y que, por ello, no estén aportando más al modelo y que podrían estar distorsionando las predicciones generando overfitting.


```{r}

summary(lm1)
summary(lm2)

lm2 = lm(formula = Fertility ~ Catholic + Agriculture +  Education + Examination +  Infant.Mortality, data = swiss)
summary(lm1)

lm3 = lm(formula = Fertility ~ Examination + Agriculture + Education  +  Catholic + Infant.Mortality, data = swiss)
summary(lm3)

MASS::stepAIC(lm1)

install.packages("mctest")
library(mctest)
omcdiag(swiss[,-1], swiss[,1])
imcdiag(swiss[,-1], swiss[,1])
install.packages("ppcor")
library(ppcor)
pcor(swiss, method = "pearson")

car::vif(lm1)

chart.Correlation(swiss, histogram=FALSE, pch=19)

```

x = model.matrix(Fertility~.,swiss)[,-c(1)] # matriz de covariables
y = swiss$Fertility              # vector respuesta 


set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)  # escoge 50% de los datos para entrenamiento
test = (-train) 
y.test = y[test]


grid = 10^seq(10,-2,length=100) #grid de 100 valores para lambda (de 10^10 a 10^{-2})
ridge.mod = glmnet(x,y,alpha=0,lambda=grid) #alpha= 0 => regresion ridge
ridge.mod

set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha=0,nfolds=5)
plot(cv.out,main="Ridge regression") # Traza la curva de validacion cruzada y las curvas de desviacion estandar superior e inferior, en funciÃ³n de los valores de lambda utilizados.
bestlambda = cv.out$lambda.min #escoge el mejor valor de lambda (el que alcanza el menor ECM - error cuadratico medio -  en la validacion cruzada)


ridge.pred = predict(ridge.mod,s=bestlambda,newx=x[test,])
mean((ridge.pred-y.test)^2)


library(psych)
pairs.panels(swiss[c(1,2,3,4,5,6)])
summary(lm1)

tt =  as.numeric( predict(lm1, as.data.frame(x[test,])))
tt_test = as.numeric(y.test)
length(tt)
length(y.test)

mean((tt -tt_test)^2)  


out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlambda) #muestra los coeficientes del modelo final

y_predicted = predict(out, as.matrix(swiss[,-1]), s=bestlambda) 


lm7 = lm(Fertility~.,swiss[train,])
summary(lm7)

lm_predicted = predict(lm7,  as.data.frame(swiss[,-1]))

lm_predicted = as.numeric(lm_predicted)

y_predicted = as.numeric(y_predicted)
value_y = swiss[,1]
sst <- sum((value_y - mean(value_y))^2)
sse <- sum((y_predicted - value_y)^2)

sse_lm <- sum((lm_predicted - value_y)^2)

# R squared
rsq <- 1 - sse / sst
rsq

rsq_lm <- 1 - sse_lm / sst

coef(out)
summary(lm1)

lm5 = lm(Fertility~., )
pairs.panels(swiss2)
summary(lm5)
vif(lm5)


head(swiss)




setCoeffs <- function(frml, weights, len){
  el <- paste0("offset(", weights[-1], "*", 
               unlist(strsplit(as.character(frml)[-(1:2)], " +\\+ +")), ")")
  el <- c(paste0("offset(rep(", weights[1], ",", len, "))"), el)                                 
  as.formula(paste(as.character(frml)[2], "~", 
                   paste(el, collapse = " + "), " + -1"))
}

frml <- Fertility ~ Catholic + Agriculture +  Education + Examination +  Infant.Mortality
mod <- lm( frml , data = swiss)
weights <- mod$coef
weights[1] <- 61.50870339
weights[2] <- 0.06658406
weights[3] <- -0.06426770
weights[4] <- -0.55076771
weights[5] <- -0.34738082
weights[5] <- 1.04925959
mod2 <- update(mod, setCoeffs(frml, weights, nrow(swiss)))

summary(mod2)
plot(mod2)
vif(out)

 
Agriculture      
Examination      
Education        
Catholic          
Infant.Mortality  

car::vif(lm1)



lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)

set.seed(1)
cv.out2=cv.glmnet(x[train,],y[train],alpha=1,nfolds=5)
plot(cv.out2,main="Lasso regression")
bestlam=cv.out2$lambda.min #igual a 20.12811
bestlam



#### Primero atacaremos por la distancia de Cook (Cook's Distance) para poder encontrar los outliers

```{r code8, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
plot(cooks.distance(lm1))
```

Consideramos como límite y n/4, como se puede ver en el gráfico, habrán varios valores que calificarán como outliers y por lo tanto se sacarán para poder estimar un nuevo modelo lineal

```{r code9, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <-swiss[-indices.cook, ]
lm2 <- lm(formula = Fertility ~ ., data = swiss)
summary(lm2)
```

El principal cambio se encuentre en que el valor de $R^2$ subio, por lo tanto al momento de graficar valores reales contra los estimados, los puntos deberían estar más pegados a la línea que pasara entre ellos.


Graficamos los Valores reales vs. los Valores estimados


```{r code10, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <-swiss[-indices.cook, ]
lm2 <- lm(formula = Fertility ~ ., data = swiss)
plot(lm2$fitted.values, swiss$Fertility)
abline(0, 1, col = 2)

```
Se pueden apreciar ahora los puntos mas cercanos a la linea roja. Por otra parte, 

Graficamos los Residuales vs. los Valores estimados

```{r code11, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <-swiss[-indices.cook, ]
lm2 <- lm(formula = Fertility ~ ., data = swiss)

plot(lm2$fitted.values, lm2$residuals)
abline(0, 0, col = 2)

```

Para descartar que estemos utilizando muchos regresores, los cuales no me estén brindando más información de la
que ya me están brindando los otros. Para arreglar esto utilizaremos el Akaike Information Criterion (AIC) con un stepwise en ambas direcciones


```{r code12, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <-swiss[-indices.cook, ]
lm2 <- lm(formula = Fertility ~ ., data = swiss)
step<-stepAIC(lm2, direction = "both")
summary(step)
```

Seguimos la recomendación y hacemos un modelo sin la variable Education. El resultado no debería
mejorar sustancialmente, en general debería quedarse igual y solamente se habrían eliminado variables que no
aportaban al modelo.


```{r code13, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <-swiss[-indices.cook, ]
swiss <- swiss[ , !colnames(swiss) %in% c("Education")]
lm3 <- lm(formula = Fertility ~ ., data = swiss)
summary(lm3)
```

Para comprobar que este cambio no ha sido significativo hacemos la tabla de ANOVA entre el modelo anterior y este último modelo.

```{r code14, exercise=TRUE}
lm1 <- lm(formula = Fertility ~ ., data = swiss)
indices.cook <- which(cooks.distance(lm1) >= 4/nrow(swiss))
swiss <- swiss[-indices.cook, ]
lm2 <- lm(formula = Fertility ~ ., data = swiss)
swiss <- swiss[ , !colnames(swiss) %in% c("Education")]
lm3 <- lm(formula = Fertility ~ ., data = swiss)
anova(lm2, lm3)
```
